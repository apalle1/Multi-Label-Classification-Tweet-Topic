{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github_TwitterTopicExtraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XayHLnfqpw8F",
        "colab_type": "code",
        "outputId": "c8520d0b-c560-49af-885c-cd24cbb2b0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import reuters\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "\n",
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tYQTywq1GP5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   To write into a pickle file - \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "import pickle\n",
        "\n",
        "data1 = {'a': [1, 2, 3],\n",
        "         'b': ('string', 'Unicode string'),\n",
        "         'c': None}\n",
        "\n",
        "selfref_list = [1, 2, 3]\n",
        "\n",
        "output = open('data.pkl', 'wb')\n",
        "\n",
        "pickle.dump(data1, output)\n",
        "pickle.dump(selfref_list, output)\n",
        "\n",
        "output.close()\n",
        "\n",
        "```\n",
        "\n",
        "*   To read from a pickled file -\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pkl_file = open('data.pkl', 'rb')\n",
        "\n",
        "data1 = pickle.load(pkl_file)\n",
        "print(data1)\n",
        "\n",
        "data2 = pickle.load(pkl_file)\n",
        "print(data2)\n",
        "\n",
        "pkl_file.close()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "*   How to print all the keys in the dictionary and their respective values?\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "mydic = {\"key 1\":\"value 1\",\"key b\":\"value b\"}\n",
        "for key in mydic:\n",
        "  print (\"the key name is \" + key + \" and its value is \" + mydic[key])\n",
        "\n",
        "mydic = {\"key 1\":\"value 1\",\"key b\":\"value b\"}\n",
        "for key, value in mydic.items():\n",
        "  print (\"the key name is \" + key + \" and its value is \" + mydic[key])\n",
        "]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TT0wABd3HGTb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. \n",
        "\n",
        "*   In natural language processing, useless words (data), are referred to as stop words. What are Stop words?\n",
        "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore.\n"
      ]
    },
    {
      "metadata": {
        "id": "Z75f9_VWpy3Q",
        "colab_type": "code",
        "outputId": "6b78ae62-9dc0-41d8-9475-ac1f0401b8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "fileName = ['electronics_train_data.data','electronics_and_car_train_data.data'\n",
        "\t,'politics_and_car_train_data.data','politics_and_travel_train_data.data'\n",
        "\t,'politics_train_data.data','car_train_data.data','travel_train_data.data','travel_and_electronics_train_data.data','travel_and_car_train_data.data']\n",
        "\n",
        "labelsFile = [['electronics'],['electronics','car'],['politics','car'],['politics','travel'],['politics'],['car'],['travel'],['travel','electronics'],['travel','car']]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t5dsvXzHpzBZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getDocument(tweetList):\n",
        "\tresult = ''\n",
        "\tfor tweet in tweetList:\n",
        "\t\tresult = result + ' '\n",
        "\t\tresult = result + tweet\n",
        "\treturn result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwrLjaoUpzHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def readPickleFiles():\n",
        "  labelData = []\n",
        "  contentData = []\n",
        "  for index in range(len(fileName)):\n",
        "    pkl_file = open('/content/gdrive/My Drive/Datasets/TwitterData/' + fileName[index],'rb')\n",
        "    getDict = pickle.load(pkl_file)\n",
        "    for user in getDict:\n",
        "      aggregatedDoc = getDocument(getDict[user])\n",
        "      contentData.append(aggregatedDoc)\n",
        "      labelData.append(labelsFile[index])\n",
        "  return labelData,contentData"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PV_SHU-7HVWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   re.compile(pattern) compiles a regular expression pattern into a regular expression object, which can be used for matching using its match() and search() methods.\n",
        "\n",
        "\n",
        "*  We wish to use regular expressions to search for pattern and to do this we're going to use meta characters.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        ".       - Any Character Except New Line\n",
        "\\d      - Digit (0-9)\n",
        "\\D      - Not a Digit (0-9)\n",
        "\\w      - Word Character (a-z, A-Z, 0-9, _)\n",
        "\\W      - Not a Word Character\n",
        "\\s      - Whitespace (space, tab, newline)\n",
        "\\S      - Not Whitespace (space, tab, newline)\n",
        "\n",
        "\\b      - Word Boundary\n",
        "\\B      - Not a Word Boundary\n",
        "^       - Beginning of a String\n",
        "$       - End of a String\n",
        "\n",
        "[]      - Matches Characters in brackets\n",
        "[^ ]    - Matches Characters NOT in brackets\n",
        "|       - Either Or\n",
        "( )     - Group\n",
        "\n",
        "Quantifiers:\n",
        "*       - 0 or More\n",
        "+       - 1 or More\n",
        "?       - 0 or One\n",
        "{3}     - Exact Number\n",
        "{3,4}   - Range of Numbers (Minimum, Maximum)\n",
        "\n",
        "#### Example ####\n",
        "emails = 'CoreyMSchafer@gmail.com, corey.schafer@university.edu, corey-321-schafer@my-work.net'\n",
        "pattern = re.compile(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
        "matches = pattern.finditer(emails)\n",
        "\n",
        "for match in matches:\n",
        "    print(match)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ew7M9WoFpzRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getOnlyWords(wordsList):\n",
        "\tpattern = re.compile('[a-zA-Z]+')\n",
        "\tresult = []\n",
        "\tfor word in wordsList:\n",
        "\t\tif pattern.match(word) != None:\n",
        "\t\t  result.append(word)\n",
        "\treturn result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w7jeuVTS1xtS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getWordsWithoutStop(sentence):\n",
        "\tsentence = sentence.lower()\n",
        "\twords = word_tokenize(sentence)\n",
        "\twithoutStopWords = [word for word in words if not word in stopWords]\n",
        "\treturn getOnlyWords(withoutStopWords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eg22vO7qHwVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   A tokenizer divides text into a sequence of tokens. The tokenizer argument should be a function that takes a string as an input.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "RUN THE BELOW COMMENTED CODE TO UNDERSTAND LOWER CASING LETTERS, TOKENIZING WORDS, REMOVING STOPWORDS, REMOVING EVERY WORD EXCEPT THOSE CONTAINING [a-zA-Z]+ \n",
        "\n",
        "def getWordsWithoutStop(sentences): \n",
        "  sentences = sentences.lower()\n",
        "  #print(sentences)\n",
        "\n",
        "  withoutStopWords = []\n",
        "  words = word_tokenize(sentences)\n",
        "  #print(words)\n",
        "\n",
        "  withoutStopWord = [word for word in words if not word in stopWords]\n",
        "  #print(withoutStopWord)\n",
        "  \n",
        "  pattern = re.compile('[a-zA-Z]+')\n",
        "  result = []\n",
        "  for word in withoutStopWord:\n",
        "    if pattern.match(word) != None:\n",
        "      result.append(word)\n",
        "  #print(result)\n",
        "  return result\n",
        "\n",
        "trainContent = ['Within minutes, the pro-government television channels were calling it a huge ‘game-changer’ but the “ten-percent quota” gambit could mean only one thing: the 5555 cabal will go to any length – do anything, say anything, promise anything – to try to win the next Lok Sabha election.', 'Never mind that the proposed constitutional amendment would certainly run afoul of the Supreme Court, but the super-clever men who constitute the 5555 clique clearly hope that large chunks of the friendly media will help the prime minister re-write the ‘narrative’ around this 10% business.', 'The 5555 crowd may well be entitled to its conceits but the democratic, liberal, progressive and secular voices owe it to the country not to get taken in by this sleight of hand.']\n",
        "vectorizer = TfidfVectorizer(tokenizer = getWordsWithoutStop)\n",
        "vectorizedTrainData = vectorizer.fit_transform(trainContent)\n",
        "vectorizer.get_feature_names()\n",
        "vectorizedTrainData.shape\n",
        "vectorizedTrainData.toarray()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aRh6lBw23Ymp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label, Content = readPickleFiles()\n",
        "trainContent, testContent, labelTrain, labelTest = train_test_split(Content, label, test_size=.2, random_state=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rsP-EABoAvMt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer = getWordsWithoutStop)\n",
        "vectorizedTrainData = vectorizer.fit_transform(trainContent)\n",
        "vectorizedTestData = vectorizer.transform(testContent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTfAk1RJH7kN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. \n",
        "Multilabel classification assigns to each sample a set of target labels. A text might be about any of religion, politics, finance or education at the same time or none of these.\n",
        "\n",
        "\n",
        "\n",
        "*   Classifier Chain - A chain C1, · · · , C|L| of binary classifiers is formed. Each classifier Cj\n",
        "in the chain is responsible for learning and predicting the binary association of label\n",
        "lj given the feature space, augmented by all prior binary relevance predictions\n",
        "in the chain l1, · · · , lj−1. The classification process begins at C1 and propagates\n",
        "along the chain: C1 determines Pr(l1|x) and every following classifier C2 · · · C|L|\n",
        "predicts P r(lj |xi, l1, . . . , lj−1). \n"
      ]
    },
    {
      "metadata": {
        "id": "jvwCpwqCqVnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multiLabelBinary = MultiLabelBinarizer()\n",
        "trainBinaryLabel = multiLabelBinary.fit_transform(labelTrain)\n",
        "testBinaryLabel = multiLabelBinary.transform(labelTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xG0PDuABqVvM",
        "colab_type": "code",
        "outputId": "f22626b7-e5ab-4d48-d584-8e3a8415104d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "classifierSVM = OneVsRestClassifier(LinearSVC())\n",
        "classifierSVM.fit(vectorizedTrainData, trainBinaryLabel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "     verbose=0),\n",
              "          n_jobs=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "lyViEhKiqVzV",
        "colab_type": "code",
        "outputId": "0867bc55-ac46-4657-9b21-1a060dfab821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "predictionsTrain = classifierSVM.predict(vectorizedTrainData)\n",
        "\n",
        "precisionTrain = precision_score(trainBinaryLabel, predictionsTrain, average = 'micro')\n",
        "print (\"Linear SVM multi-label Train micro,\")\n",
        "print (precisionTrain)\n",
        "\n",
        "precisionTrain = precision_score(trainBinaryLabel, predictionsTrain, average = 'macro')\n",
        "print (\"Linear SVM multi-label Train macro,\")\n",
        "print (precisionTrain)\n",
        "\n",
        "predictionsTest = classifierSVM.predict(vectorizedTestData)\n",
        "\n",
        "precisionTest = precision_score(testBinaryLabel, predictionsTest, average = 'micro')\n",
        "print (\"Linear SVM multi-label Test micro,\")\n",
        "print (precisionTest)\n",
        "\n",
        "precisionTest = precision_score(testBinaryLabel, predictionsTest, average = 'macro')\n",
        "print (\"Linear SVM multi-label Test macro,\")\n",
        "print (precisionTest)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear SVM multi-label Train micro,\n",
            "0.998556894436828\n",
            "Linear SVM multi-label Train macro,\n",
            "0.9986854981779145\n",
            "Linear SVM multi-label Test micro,\n",
            "0.9766949152542372\n",
            "Linear SVM multi-label Test macro,\n",
            "0.9776718388111761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x8e8FmMwqV2u",
        "colab_type": "code",
        "outputId": "97590498-5388-43bb-dfbf-d069a8b63a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "cell_type": "code",
      "source": [
        "randomChainClassifier = [ClassifierChain(LinearSVC(), order='random', random_state=i) for i in range(20)]\n",
        "for chain in randomChainClassifier:\n",
        "\tchain.fit(vectorizedTrainData,trainBinaryLabel)\n",
        "\n",
        "predictionsTrain = np.array([chain.predict(vectorizedTrainData) for chain in randomChainClassifier])\n",
        "\n",
        "predictionsTrain = predictionsTrain.mean(axis=0)\n",
        "threshold = [.1,.2,.3,.4,.5]\n",
        "maxThreshold = 0.0\n",
        "accuracyMax = 0.0\n",
        "\n",
        "for thres in threshold:\n",
        "\tcurPredictionsTrain = (predictionsTrain >= thres)\n",
        "\tprecisionTrain = precision_score(trainBinaryLabel, curPredictionsTrain, average = 'micro')\n",
        "\tprint (\"ClassifierChain multi-label Train micro,\")\n",
        "\tprint (precisionTrain)\n",
        "\tprecisionTrain = precision_score(trainBinaryLabel, curPredictionsTrain, average = 'macro')\n",
        "\tprint (\"ClassifierChain multi-label Train macro,\")\n",
        "\tprint (precisionTrain)\n",
        "\tif accuracyMax < precisionTrain:\n",
        "\t\taccuracyMax = precisionTrain\n",
        "\t\tmaxThreshold = thres\n",
        "\n",
        "predictionsTest = np.array([chain.predict(vectorizedTestData) for chain in randomChainClassifier])\n",
        "predictionsTest = predictionsTest.mean(axis=0)\n",
        "predictionsTest = (predictionsTest >= maxThreshold)\n",
        "\n",
        "precisionTest = precision_score(testBinaryLabel, predictionsTest, average = 'micro')\n",
        "print (\"ClassifierChain multi-label Test micro,\")\n",
        "print (precisionTest)\n",
        "\n",
        "precisionTest = precision_score(testBinaryLabel, predictionsTest, average = 'macro')\n",
        "print (\"ClassifierChain multi-label Test macro,\")\n",
        "print (precisionTest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ClassifierChain multi-label Train micro,\n",
            "0.9982338523644751\n",
            "ClassifierChain multi-label Train macro,\n",
            "0.998363272570492\n",
            "ClassifierChain multi-label Train micro,\n",
            "0.9983418045492232\n",
            "ClassifierChain multi-label Train macro,\n",
            "0.9984416667591766\n",
            "ClassifierChain multi-label Train micro,\n",
            "0.9984137284591535\n",
            "ClassifierChain multi-label Train macro,\n",
            "0.9985089431608121\n",
            "ClassifierChain multi-label Train micro,\n",
            "0.9984496683011249\n",
            "ClassifierChain multi-label Train macro,\n",
            "0.9985838039417672\n",
            "ClassifierChain multi-label Train micro,\n",
            "0.9984852856318522\n",
            "ClassifierChain multi-label Train macro,\n",
            "0.9986184804420662\n",
            "ClassifierChain multi-label Test micro,\n",
            "0.9694485842026825\n",
            "ClassifierChain multi-label Test macro,\n",
            "0.9700424010247494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M9IQqNRhqVsQ",
        "colab_type": "code",
        "outputId": "a1f746e7-4893-4f53-da83-5bff1aa88277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "testSet = ['A great blend of handheld comfort and a big, gorgeous OLED screen. Rear telephoto camera outshoots the 8 Plus in low light, and the front camera snaps impressive portrait selfies. Face ID generally works fine.'\n",
        ",'Standard equipment for all Fit models includes a backup camera, automatic headlights, Bluetooth, LED taillights and a center console with an armrest.'\n",
        ",'The term political statement is used to refer to any act or non-verbal form of communication that is intended to influence a decision to be made for or by a political party.'\n",
        ",'The democrats are not happy about the trip to australia','I bought a bunch of automatic transmissions when Im travelling to to another country','I will be attending a birthday party today']\n",
        "\n",
        "vectorizedTestData = vectorizer.transform(testSet)\n",
        "predictionsTest = classifierSVM.predict(vectorizedTestData)\n",
        "print (\"Linear SVM multi-label manual input,\")\n",
        "print (multiLabelBinary.inverse_transform(predictionsTest))\n",
        "\n",
        "predictionsTest = np.array([chain.predict(vectorizedTestData) for chain in randomChainClassifier])\n",
        "predictionsTest = predictionsTest.mean(axis=0)\n",
        "predictionsTest = (predictionsTest >= maxThreshold)\n",
        "\n",
        "print (\"ClassifierChain multi-label manual input,\")\n",
        "print (multiLabelBinary.inverse_transform(predictionsTest))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear SVM multi-label manual input,\n",
            "[('electronics',), ('electronics',), ('politics',), ('travel',), ('car',), ('politics',)]\n",
            "ClassifierChain multi-label manual input,\n",
            "[('electronics',), ('electronics',), ('politics',), ('travel',), ('car', 'travel'), ('politics',)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}